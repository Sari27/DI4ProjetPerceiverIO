{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'perceiver'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_337/1193494534.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mperceiver\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mposition_encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mio_processors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'perceiver'"
     ]
    }
   ],
   "source": [
    "#@title Import\n",
    "from typing import Union\n",
    "\n",
    "# Haiku permet aux utilisateurs d'utiliser des modèles de POO tout en permettant un accès complet aux transformations de fonctions pures de JAX pour les reseaux neuronaux.\n",
    "import haiku as hk\n",
    "# JAX est concue pour le calcul numérique à haute performance, notamment pour la recherche en apprentissage automatique.\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# NumPy ajoute un support pour les grands tableaux multidimensionnels et les matrices, ainsi qu'une grande collection de fonctions mathématiques pour opérer sur ces tableaux.\n",
    "import numpy as np\n",
    "# Pickle est un module permettant la (dé)sérialisation\n",
    "import pickle\n",
    "\n",
    "import perceiver\n",
    "import position_encoding\n",
    "import io_processors\n",
    "import bytes_tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Load parameters from checkpoint\n",
    "#!wget -O language_perceiver_io_bytes.pickle https://storage.googleapis.com/perceiver_io/language_perceiver_io_bytes.pickle\n",
    "\n",
    "# On deserialise les valeurs des hyperparametres du modele.\n",
    "with open(\"language_perceiver_io_bytes.pickle\", \"rb\") as f:\n",
    "  params = pickle.loads(f.read())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Model config\n",
    "D_MODEL = 768\n",
    "D_LATENTS = 1280\n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "encoder_config = dict(\n",
    "    num_self_attends_per_block=26,\n",
    "    num_blocks=1,\n",
    "    z_index_dim=256,\n",
    "    num_z_channels=D_LATENTS,\n",
    "    num_self_attend_heads=8,\n",
    "    num_cross_attend_heads=8,\n",
    "    qk_channels=8 * 32,\n",
    "    v_channels=D_LATENTS,\n",
    "    use_query_residual=True,\n",
    "    cross_attend_widening_factor=1,\n",
    "    self_attend_widening_factor=1)\n",
    "\n",
    "decoder_config = dict(\n",
    "    output_num_channels=D_LATENTS,\n",
    "    position_encoding_type='trainable',\n",
    "    output_index_dims=MAX_SEQ_LEN,\n",
    "    num_z_channels=D_LATENTS,\n",
    "    qk_channels=8 * 32,\n",
    "    v_channels=D_MODEL,\n",
    "    num_heads=8,\n",
    "    final_project=False,\n",
    "    use_query_residual=False,\n",
    "    trainable_position_encoding_kwargs=dict(num_channels=D_MODEL))\n",
    "\n",
    "# The tokenizer is just UTF-8 encoding (with an offset)\n",
    "tokenizer = bytes_tokenizer.BytesTokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Decoding Perceiver Model\n",
    "def apply_perceiver(\n",
    "    inputs: jnp.ndarray, input_mask: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Runs a forward pass on the Perceiver.\n",
    "\n",
    "  Args:\n",
    "    inputs: input bytes, an int array of shape [B, T]\n",
    "    input_mask: Array of shape indicating which entries are valid and which are\n",
    "      masked. A truthy value indicates that the entry is valid.\n",
    "\n",
    "  Returns:\n",
    "    The output logits, an array of shape [B, T, vocab_size].\n",
    "  \"\"\"\n",
    "  assert inputs.shape[1] == MAX_SEQ_LEN\n",
    "\n",
    "  # Dimensions choisies des caracteristiques latentes\n",
    "  embedding_layer = hk.Embed(\n",
    "      vocab_size=tokenizer.vocab_size,\n",
    "      embed_dim=D_MODEL)\n",
    "  # Encodage et reduction de l'input a l'espace des caracteristiques latentes\n",
    "  embedded_inputs = embedding_layer(inputs)\n",
    "\n",
    "  # Taille de la reduction de l'input a l'espace des caracteristiques latentes\n",
    "  batch_size = embedded_inputs.shape[0]\n",
    "\n",
    "  # Encodage de la partie masquee de l'input pour que le transformer sache la localiser\n",
    "  input_pos_encoding = perceiver.position_encoding.TrainablePositionEncoding(\n",
    "      index_dim=MAX_SEQ_LEN, num_channels=D_MODEL)\n",
    "  # Fusion de la reduction de l'input et de la position encodee de la partie masquee\n",
    "  embedded_inputs = embedded_inputs + input_pos_encoding(batch_size)\n",
    "  perceiver_mod = perceiver.Perceiver(\n",
    "      encoder=perceiver.PerceiverEncoder(**encoder_config),\n",
    "      decoder=perceiver.BasicDecoder(**decoder_config))\n",
    "  # Stockage dans output du resultat de l'execution du modele sur l'input encode\n",
    "  output_embeddings = perceiver_mod(\n",
    "      embedded_inputs, is_training=False, input_mask=input_mask, query_mask=input_mask)\n",
    "\n",
    "  # Redimensionnement et decodage de l'output\n",
    "  logits = io_processors.EmbeddingDecoder(\n",
    "      embedding_matrix=embedding_layer.embeddings)(output_embeddings)\n",
    "  return logits\n",
    "\n",
    "# La transformation permettra plus tard a la fonction de lui passer des parametres pre-enregistres\n",
    "apply_perceiver = hk.transform(apply_perceiver).apply"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Input sous format initial\n",
    "input_str = \"This is the best thing that happened to me!\"\n",
    "# On marque les coordonnees a predire\n",
    "input_tokens_positions = [\n",
    "    input_str.index(\" is\"),input_str.index(\"the best\")\n",
    "]\n",
    "# vectorisation en bytes de l'input, sous forme d'entiers\n",
    "input_tokens = tokenizer.to_int(input_str)\n",
    "\n",
    "# Note that the model performs much better if the masked chunk\n",
    "# starts with a space.\n",
    "# On masque la partie de l'input a predire avec une constante de tokenizer\n",
    "input_tokens[input_tokens_positions[0]:input_tokens_positions[1]] = tokenizer.mask_token\n",
    "print(\"Tokenized string without masked bytes:\")\n",
    "print(tokenizer.to_string(input_tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Pad and reshape inputs\n",
    "#print(input_tokens)\n",
    "# inputs est le tableau d'entiers avec une dimension supplementaire\n",
    "# c'est comme si on mettait le tableau inputs a l'interieur d'un nouveau tableau\n",
    "inputs = input_tokens[None]\n",
    "#print(inputs)\n",
    "# input_mask est l'equivalent unitaire de inputs :\n",
    "# pour toutes les valeurs d'inputs, on met un 1\n",
    "input_mask = np.ones_like(inputs)\n",
    "#print(input_mask)\n",
    "\n",
    "# Agrandit les tableaux inputs et input_mask pour qu'ils aient\n",
    "# une taille de max_sequence_length, et les remplit avec des 0\n",
    "def pad(max_sequence_length: int, inputs, input_mask):\n",
    "  input_len = inputs.shape[1]\n",
    "  assert input_len <= max_sequence_length\n",
    "  pad_len = max_sequence_length - input_len\n",
    "  padded_inputs = np.pad(\n",
    "      inputs,\n",
    "      pad_width=((0, 0), (0, pad_len)),\n",
    "      constant_values=tokenizer.pad_token)\n",
    "  padded_mask = np.pad(\n",
    "      input_mask,\n",
    "      pad_width=((0, 0), (0, pad_len)),\n",
    "      constant_values=0)\n",
    "  return padded_inputs, padded_mask\n",
    "\n",
    "inputs, input_mask = pad(MAX_SEQ_LEN, inputs, input_mask)\n",
    "#print(inputs)\n",
    "#print(input_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(1)  # Unused\n",
    "\n",
    "out = apply_perceiver(params, rng=rng, inputs=inputs, input_mask=input_mask)\n",
    "\n",
    "# la prediction sous forme d'entiers est constituee des valeurs maximales\n",
    "# aux coordonnees du masque sur la 1ere dimension de out\n",
    "#print(out)\n",
    "masked_tokens_predictions = out[0, input_tokens_positions[0]:input_tokens_positions[1]].argmax(axis=-1)\n",
    "print(\"Greedy predictions:\")\n",
    "print(masked_tokens_predictions) #predictions sous formes de caracteres\n",
    "print()\n",
    "print(\"Predicted string:\")\n",
    "print(tokenizer.to_string(masked_tokens_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}